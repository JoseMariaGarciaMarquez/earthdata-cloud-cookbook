[
  {
    "href": "index.html#the-new-cloud-paradigm",
    "title": "Welcome",
    "section": "The new cloud paradigm",
    "text": "NASA Distributed Active Archive Centers (DAACs) are in the process of moving their data holdings to the cloud (orange in diagram). In the new paradigm, the data storage (green in diagram) and DAAC-provided tools and services built on top of the data are co-located in the Earthdata Cloud (hosted in AWS cloud).\n Illustration by Catalina Oaida, PO.DAAC\nAs this data migration occurs, DAACs will have more information about how users can access data. For example, the Cloud Data page at PO.DAAC offers access to resources to help guide data users in discovering, accessing, and utilizing cloud data. During this transition, some data will continue to be available from the traditional on premise archive, while some data will also be available from and within the Earthdata Cloud."
  },
  {
    "href": "files/appendix/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "All the NASA (and more) acronyms, uncovered\nCOG: Cloud-optimized geo-tiff\nCTL: Command Line Tool"
  },
  {
    "href": "files/appendix/index.html",
    "title": "Appendix",
    "section": "",
    "text": "More cool stuff."
  },
  {
    "href": "files/appendix/about.html",
    "title": "About",
    "section": "",
    "text": "About this project and who we are"
  },
  {
    "href": "files/transformations/harmony-api.html",
    "title": "Harmony API",
    "section": "",
    "text": "text"
  },
  {
    "href": "files/transformations/earthdata-search.html",
    "title": "Earthdata Search UI",
    "section": "",
    "text": "text"
  },
  {
    "href": "files/transformations/index.html",
    "title": "NASA Cloud Data Transformations",
    "section": "",
    "text": "text"
  },
  {
    "href": "files/transformations/opendap-cloud.html",
    "title": "OPeNDAP in the Cloud",
    "section": "",
    "text": "OPeNDAP in the Cloud (OPeNDAP Hydrax)"
  },
  {
    "href": "files/transformations/harmonypy.html",
    "title": "Harmony Py Library",
    "section": "",
    "text": "Amy - should we include discussion/resource on using the HarmonyPy library?"
  },
  {
    "href": "files/access/access-something-specific.html",
    "title": "EarthData Cloud Cookbook",
    "section": "",
    "text": "Jupyter Notebook that demonstrates how to access data\nTODO: credit where this came from\n\nprint(\"hello world\")\nprint(\"hello universe\")\n\nhello world\nhello universe"
  },
  {
    "href": "files/access/direct-in-region.html",
    "title": "Direct in-region (scripted, not cloud)",
    "section": "",
    "text": "text here"
  },
  {
    "href": "files/access/earthdata-search.html",
    "title": "Earthdata Search",
    "section": "",
    "text": "Earthdata Search (UI)"
  },
  {
    "href": "files/access/cof-via-harmony.html",
    "title": "COF via Earthdata Harmony",
    "section": "",
    "text": "In COF (zarr) via Earthdata Harmony API (services) (scripted, in cloud)"
  },
  {
    "href": "files/access/index.html",
    "title": "NASA Cloud Data Access",
    "section": "",
    "text": "Some background here about access."
  },
  {
    "href": "files/access/opendap-cloud.html",
    "title": "OPeNDAP in the Cloud",
    "section": "",
    "text": "OPeNDAP in the Cloud (OPeNDAP Hyrax)"
  },
  {
    "href": "files/access/download-to-local.html",
    "title": "Download to local",
    "section": "",
    "text": "Download to local machine"
  },
  {
    "href": "files/access/data-access-example.html#like-a-notebook-experience-but-a-text-file-in-a-visual-editor",
    "title": "Notebook demonstrating data access",
    "section": "Like a notebook experience but a text file in a visual editor",
    "text": "1+1\n2+2\n\n4\n\n\nJust working on narrative, can set up caching like Jupyterbook so that it doesn’t execute the code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nthis is an aid to your composition as you go, but then you still render"
  },
  {
    "href": "files/processing/processing.html",
    "title": "Processing",
    "section": "",
    "text": "Processing details here"
  },
  {
    "href": "files/contributing/setup.html#overview",
    "title": "Initial Setup",
    "section": "Overview",
    "text": "This will walk you through the setup required to run Quarto to contribute to our book. You can always refer to quarto.org for the most up-to-date and more detailed information.\nSetup for our Cookbook is complete when you can answer yes to the following:\n\n“Can you run Jupyter?”\n“Can you install Quarto and render a hello world doc?”\n“Can you restore the virtual environment and render our Cookbook?”\n\nThe Jupyter and Quarto questions can be answered in any order, but must preceed working with our Cookbook.\n\n\n\n\n\n\nThe following is written for the command line, and we can update with other scenarios as we go.\n\n\n\n\n\n\n\n\n\nThe current set-up is for our book that has only Python, not R code. We’ll add R setup instructions after we have everyone working with our current setup.\n\n\n\n\n\n\n\n\n\nComing up we’ll also outline other ways to contribute and review the Cookbook through GitHub on the browser."
  },
  {
    "href": "files/contributing/setup.html#jupyter",
    "title": "Initial Setup",
    "section": "Jupyter",
    "text": "First, install Jupyter (learn more at jupyter.org/install).\npip install jupyter\nCheck to make sure it installed properly, launch jupyterlab, which will open in your browser:\njupyter-lab\nIf you need to first install Python, please do so from python.org/downloads, not from a conda installation.\nWindows Users: Please ensure you are using Python from python.org/downloads and add to PATH as showed below."
  },
  {
    "href": "files/contributing/setup.html#quarto",
    "title": "Initial Setup",
    "section": "Quarto",
    "text": "Download the Quarto command line interface (CLI) for your operating system and install following the prompts:\nhttps://github.com/quarto-dev/quarto-cli/releases/latest\nCheck to make sure Quarto installed properly with the following. It should return information about Quarto’s commands.\nquarto help"
  },
  {
    "href": "files/contributing/setup.html#clone-the-cookbook-from-github",
    "title": "Initial Setup",
    "section": "Clone the Cookbook from GitHub",
    "text": "Now clone our book:\ngit clone https://github.com/NASA-Openscapes/earthdata-cloud-cookbook \ncd earthdata-cloud-cookbook\nIf you need to set up GitHub see instructions [TODO]"
  },
  {
    "href": "files/contributing/setup.html#virtual-environment",
    "title": "Initial Setup",
    "section": "Virtual environment",
    "text": "You’ll now need to activate and install the virtual environment that has all the Python (and soon, R) libraries that are required for our Cookbook. See https://quarto.org/docs/getting-started/installation.html#virtual-environments for more background on the following steps.\nActivation differs slightly depending on which platform / shell you are using:\nBash/ZshWindowsPowerShell\n\n\nsource .venv/bin/activate\n\n\n.venv\\Scripts\\activate.bat\n\n\n.venv\\Scripts\\Activate.ps1\nNote that you may receive an error about running scripts being disabled when activating within PowerShell. If you get this error then execute the following command:\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n\n\nInstallation is done with pip from the requirements.txt file:\npip install -r requirements.txt\nSidenote: More details about environments, from A Guide to Python’s Virtual Environments (Sarmiento 2019):\n\nA virtual environment is a Python tool for dependency management and project isolation. They allow Python site packages (third party libraries) to be installed locally in an isolateddirectory for a particular project, as opposed to being installed globally (i.e. as part of a system-wide Python)."
  },
  {
    "href": "files/contributing/setup.html#youre-all-set",
    "title": "Initial Setup",
    "section": "You’re all set!",
    "text": "Now move on to the workflow chapter."
  },
  {
    "href": "files/contributing/workflow.html#workflow-for-contributing-to-our-cookbook",
    "title": "Workflow",
    "section": "Workflow for contributing to our Cookbook",
    "text": "Your Quarto workflow can be from the Command Line (bash), Python, or R. Its book chapters can be many file types, including .md , .ipynb, .Rmd, and .qmd. In all cases narrative and prose can be written in markdown, and chapters without code to execute can be written in .md. This workflow can streamline collaboration for scientific & technical writing across programming languages.\n\n\n\n\n\n\nIn progress!"
  },
  {
    "href": "files/contributing/workflow.html#github-workflow",
    "title": "Workflow",
    "section": "GitHub Workflow",
    "text": "First let’s talk about the GitHub part of the workflow. The main steps of working with GitHub are to pull, (work), stage, commit, (pull), and push. A great resource on GitHub setup and collaboration: https://happygitwithr.com/ (R-focused but fantastic philosophy and bash commands for setup, workflows, and collaboration).\nWe’re going to use branches and follow a shared workflow: create a branch, work in your branch and commit regularly, and push to github often. When you’re ready, create a pull request and we’ll merge it into the main branch.\n\nBranch setup and workflow\nCreate a new branch, then switch to that branch to work in. Then, connect it to github.com by pushing it “upstream” to the “origin repository.” git checkout -b branch-name is a 1-step approach for git branch branch-name git checkout branch-name (read more).\n## create and switch to new branch\ngit checkout -b branch-name\nCheck which branch you’re on anytime by not specifying a branch name:\ngit branch\nTime for your Quarto workflow – see below. Then when you’re ready after you render the whole book, push back to GitHub. First you have to connect your branch to github.com (the “upstream origin”) (-u below is short for --set-upstream)\n## commit regularly as you work\ngit add --all \ngit commit -m \"my commit message here\" \n\n## connect your branch to github.com and push\ngit push -u origin branch-name\nNow it’s on github, in a separate branch from main. You can go to https://github.com/nasa-openscapes/quartobook-test and do a pull request, and tag someone to review (depending on what you’ve done and what we’ve talked about).\nTODO: Let’s discuss this:\n\nWhen the pull request is merged, delete the branch on github.com\nThen also delete the branch locally:\n\ngit checkout main # switch to the main branch\ngit branch -d branch-hame"
  },
  {
    "href": "files/contributing/workflow.html#quarto-workflow",
    "title": "Workflow",
    "section": "Quarto Workflow",
    "text": "OK now we are setup and ready to work! The thing to do first is to “serve” (build) the book to make sure everything’s working. (It’s called “serve” because it’s really a website that looks like a book).\nThe overall workflow will be to serve the book at the beginning, make edits and render your .Rmd/.qmd pages to view your edits as you go (.md are automatic) and then when you’re ready to publish, you render the book with an additional command. Learn more about rendering here: https://quarto.org/docs/computations/running-code.html#rendering. From J.J. at RStudio:\n\nFor .Rmd and .qmd files you need to render them (.md updates show on save because there is no render step). The reason Quarto doesn’t render .Rmd and .qmd on save is that render could (potentially) be very long running and that cost shouldn’t be imposed on you whenever you save.\nHere we are talking about the age old debate of whether computational markdown should be rendered on save when running a development server. Quarto currently doesn’t do this to give the user a choice between an expensive render and a cheap save. See: https://quarto.org/docs/websites/website-basics.html#workflow.\n\nThe structure of the book is written in _quarto.yml. More description on this upcoming.\n\nCommand Line/Python\nTo serve the book, run the following:\nquarto serve\nPaste the url from the console into your browser to see your updates.\nContinue working, the .md files will refresh live! To refresh files with executable code, type:\nquarto render jupyter-document.ipynb\nTo render and the whole book before publishing:\nquarto render\n\n\nR\nTo the serve the book from R:\nquarto::quarto_serve()\nContinue working, the .md files will refresh live! To refresh files with executable code, type:\nquarto::quarto_render(\"filename.ipynb\")\nTo render and the whole book before publishing:\nquarto::quarto_render()"
  },
  {
    "href": "files/contributing/workflow.html#updating-the-environment",
    "title": "Workflow",
    "section": "Updating the environment",
    "text": "TODO!\nFrom R: As we develop and add more package dependencies, re-run renv::snapshot() to update the environment."
  },
  {
    "href": "files/contributing/workflow.html#if-youre-testing-code",
    "title": "Workflow",
    "section": "If you’re testing code",
    "text": "The workflow there would be that a user decides that they will be the only one who runs the notebook. Adding execute: false basically means that Quarto never runs the code, but the user of course still can interactively in Jupyter."
  },
  {
    "href": "files/contributing/index.html#intro",
    "title": "Contributing",
    "section": "Intro",
    "text": "This Section is about how our Openscapes Cohort collaborates to create this book, with an eye towards how others could collaborate with us in the future.\nTODO: Fork/borrow from the awesome work at The Turing Way: https://the-turing-way.netlify.app/community-handbook/community-handbook.html"
  },
  {
    "href": "files/contributing/index.html#quarto",
    "title": "Contributing",
    "section": "Quarto",
    "text": "We’re making the EarthData Cloud Cookbook with Quarto: quarto.org. Quarto makes collaborating to create technical documentation streamlined because we work in plain text documents that can have executable code (Python, R) and are rendered using Jupyter and Knitr engines.\nWhat is Quarto? Quarto builds from what RStudio learned from RMarkdown but enables different engines (Jupyter and knitr). It is both a Command Line Tool and R package. .qmd is a new filetype like .Rmd — meaning it’s a text file but coupled with an engine can execute code and be rendered as html, pdf, word, and beyond — but for other languages like Python. Quarto can convert .ipynb files to and from .md and .qmd easily so you can develop and publish with collaborators that have different workflows. Once the book is “served” locally, .md files auto-update as you edit, and files with executable code can be rendered individually, and the behavior of different code chunks can be controlled and cached.\n(Note: with Quarto, e-books and websites are very similarly structured, with e-books being set up for numbered chapters and references and websites set up for higher number of pages and organization. We can talk about our book as a book even as we explore whether book/website better suits our needs. This is assigned in _quarto.yml, as we’ll explore later)."
  },
  {
    "href": "files/discovery/Navigate_CMR_STAC.html#introduction-to-the-cmr-stac-api",
    "title": "EarthData Cloud Cookbook",
    "section": "2.1 Introduction to the CMR-STAC API ",
    "text": "What is STAC?\n\nSTAC is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data. ### Four STAC Specifications: 1. STAC API\n2. STAC Catalog\n3. STAC Collection\n4. STAC Item\n#### In the section below, we will walk through an example of each specification. For additional information, check out: https://stacspec.org/.\n\n\n\n1. STAC API: Endpoint that enables the querying of STAC items.\n\nBelow, set the CMR-STAC API Endpoint to a variable, and use the requests package to send a GET request to the endpoint, and set the response to a variable.\n\nstac = 'https://cmr.earthdata.nasa.gov/stac/' # CMR-STAC API Endpoint\nstac_response = r.get(stac).json()            # Call the STAC API endpoint\nfor s in stac_response: print(s)\n\nid\ntitle\nstac_version\ntype\ndescription\nlinks\n\n\n\nprint(f\"You are now using the {stac_response['id']} API (STAC Version: {stac_response['stac_version']}). {stac_response['description']}\")\nprint(f\"There are {len(stac_response['links'])} STAC catalogs available in CMR.\")\n\nYou are now using the stac API (STAC Version: 1.0.0-rc.4). This is the landing page for CMR-STAC. Each provider link contains a STAC endpoint.\nThere are 46 STAC catalogs available in CMR.\n\n\n\n\nYou will notice above that the CMR-STAC API contains many different endpoints–not just from NASA LP DAAC, but also contains endpoints for other NASA ESDIS DAACs.\n\n\n\n2. STAC Catalog: Contains a JSON file of links that organize all of the collections available.\n\nBelow, search for LP DAAC Catalogs, and print the information contained in the Catalog that we will be using today, LPCLOUD.\n\nstac_lp = [s for s in stac_response['links'] if 'LP' in s['title']]  # Search for only LP-specific catalogs\n\n# LPCLOUD is the STAC catalog we will be using and exploring today\nlp_cloud = r.get([s for s in stac_lp if s['title'] == 'LPCLOUD'][0]['href']).json()\nfor l in lp_cloud: print(f\"{l}: {lp_cloud[l]}\")\n\nid: LPCLOUD\ntitle: LPCLOUD\ndescription: Root catalog for LPCLOUD\ntype: Catalog\nstac_version: 1.0.0-rc.4\nlinks: [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Provider catalog', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac/', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'collections', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections', 'title': 'Provider Collections', 'type': 'application/json'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'GET'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'POST'}, {'rel': 'conformance', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance', 'title': 'Conformance Classes', 'type': 'application/geo+json'}, {'rel': 'service-desc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/openapi.yaml', 'title': 'OpenAPI Doc', 'type': 'application/vnd.oai.openapi+json;version=3.0'}, {'rel': 'service-doc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/index.html', 'title': 'HTML documentation', 'type': 'text/html'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5', 'type': 'application/json'}]\nconformsTo: ['https://api.stacspec.org/v1.0.0-beta.1/core', 'https://api.stacspec.org/v1.0.0-beta.1/item-search', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#fields', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#query', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#sort', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#context', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/core', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/oas30', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/geojson']\n\n\n\n\nBelow, print the links contained in the LP CLOUD STAC Catalog:\n\nlp_links = lp_cloud['links']\nfor l in lp_links: \n    try: \n        print(f\"{l['href']} is the {l['title']}\")\n    except:\n        print(f\"{l['href']}\")       \n\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD is the Provider catalog\nhttps://cmr.earthdata.nasa.gov/stac/ is the Root catalog\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections is the Provider Collections\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance is the Conformance Classes\nhttps://api.stacspec.org/v1.0.0-beta.1/openapi.yaml is the OpenAPI Doc\nhttps://api.stacspec.org/v1.0.0-beta.1/index.html is the HTML documentation\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5\n\n\n\n\n\n3. STAC Collection: Extension of STAC Catalog containing additional information that describe the STAC Items in that Collection.\n\nBelow, get a response from the LPCLOUD Collection and print the information included in the response.\n\nlp_collections = [l['href'] for l in lp_links if l['rel'] == 'collections'][0]  # Set collections endpoint to variable\ncollections_response = r.get(f\"{lp_collections}\").json()                        # Call collections endpoint\nprint(f\"This collection contains {collections_response['description']} ({len(collections_response['collections'])} available)\")\n\nThis collection contains All collections provided by LPCLOUD (3 available)\n\n\n\n\nAs of March 3, 2021, there are three collections available, and more will be added in the future.\n\n\nPrint out one of the collections:\n\ncollections = collections_response['collections']\ncollections[1]\n\n{‘id’: ‘HLSL30.v1.5,’ ‘stac_version’: ‘1.0.0-rc.4,’ ‘license’: ‘not-provided,’ ‘title’: ‘HLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5,’ ‘type’: ‘Collection,’ ‘description’: ‘PROVISIONAL - The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 (https://doi.org/10.5067/HLS/HLSS30.015) and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are “stackable” for time series analysis.HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 10 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. For a more detailed description of the individual bands provided in the HLSL30 product, please see the User Guide (https://lpdaac.usgs.gov/documents/878/HLS_User_Guide_V15_provisional.pdf).’ ‘links’: [{‘rel’: ‘self,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5,’ ‘title’: ‘Info about this collection,’ ‘type’: ‘application/json’}, {‘rel’: ‘root,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac,’ ‘title’: ‘Root catalog,’ ‘type’: ‘application/json’}, {‘rel’: ‘parent,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD,’ ‘title’: ‘Parent catalog,’ ‘type’: ‘application/json’}, {‘rel’: ‘items,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5/items,’ ‘title’: ‘Granules in this collection,’ ‘type’: ‘application/json’}, {‘rel’: ‘about,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/C1711972753-LPCLOUD.html,’ ‘title’: ‘HTML metadata for collection,’ ‘type’: ‘text/html’}, {‘rel’: ‘via,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/C1711972753-LPCLOUD.json,’ ‘title’: ‘CMR JSON metadata for collection,’ ‘type’: ‘application/json’}], ‘extent’: {‘crs’: ‘http://www.opengis.net/def/crs/OGC/1.3/CRS84,’ ‘spatial’: {‘bbox’: [[-180, -90, 180, 90]]}, ‘trs’: ‘http://www.opengis.net/def/uom/ISO-8601/0/Gregorian,’ ‘temporal’: {‘interval’: [[‘2013-04-11T00:00:00.000Z,’ None]]}}}\n\n\n\n\nIn CMR, id is used to query by a specific product, so be sure to save the ID for the HLS S30 and L30 V1.5 products below:\n\n# Search available collections for HLS and print them out\nhls_collections = [c for c in collections if 'HLS' in c['title']]\nfor h in hls_collections: print(f\"{h['title']} has an ID (shortname) of: {h['id']}\")\n\nHLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5 has an ID (shortname) of: HLSL30.v1.5\nHLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30 m V1.5 has an ID (shortname) of: HLSS30.v1.5\n\n\n\nNote that the “id” shortname is in the format: productshortname.vVVV (where VVV = product version)\n\n\n\nExplore the attributes contained in the HLSS30 Collection.\n\ns30 = [h for h in hls_collections if h['id'] == 'HLSS30.v1.5'][0]  # Grab HLSS30 collection\nfor s in s30['extent']: print(f\"{s}: {s30['extent'][s]}\")          # Check out the extent of this collection\n\ncrs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\nspatial: {'bbox': [[-180, -90, 180, 90]]}\ntrs: http://www.opengis.net/def/uom/ISO-8601/0/Gregorian\ntemporal: {'interval': [['2014-04-03T00:00:00.000Z', None]]}\n\n\n\n\nSo here we can see that the extent is global, and can also see the temporal range–where “None” means on-going or to present.\n\nprint(f\"HLS S30 Start Date is: {s30['extent']['temporal']['interval'][0][0]}\")\ns30_id = s30['id']\n\nHLS S30 Start Date is: 2014-04-03T00:00:00.000Z\n\n\n\n\nNext, explore the attributes of the HLSL30 collection.\n\nl30 = [h for h in hls_collections if h['id'] == 'HLSL30.v1.5'][0]     # Grab HLSL30 collection\nfor l in l30['extent']: print(f\"{l}: {l30['extent'][l]}\")             # Check out the extent of this collection\nprint(f\"HLS L30 Start Date is: {l30['extent']['temporal']['interval'][0][0]}\")\nl30_id = l30['id']\n\ncrs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\nspatial: {'bbox': [[-180, -90, 180, 90]]}\ntrs: http://www.opengis.net/def/uom/ISO-8601/0/Gregorian\ntemporal: {'interval': [['2013-04-11T00:00:00.000Z', None]]}\nHLS L30 Start Date is: 2013-04-11T00:00:00.000Z\n\n\n\n\nAbove, notice that the L30 product has a different start date than the S30 product.\n\n\n\n4. STAC Item: Represents data and metadata assets that are spatiotemporally coincident\n\nBelow, query the HLSS30 collection for items and return the first item in the collection.\n\n# Below, go through all links in the collection and return the link containing the items endpoint\ns30_items = [s['href'] for s in s30['links'] if s['rel'] == 'items'][0]  # Set items endpoint to variable\ns30_items\n\n‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5/items’\n\n\n\ns30_items_response = r.get(f\"{s30_items}\").json()                        # Call items endpoint\ns30_item = s30_items_response['features'][0]                             # select first item (10 items returned by default)\ns30_item\n\n{‘type’: ‘Feature,’ ‘id’: ‘G1969487860-LPCLOUD,’ ‘stac_version’: ‘1.0.0-rc.4,’ ‘stac_extensions’: [‘https://stac-extensions.github.io/eo/v1.0.0/schema.json’], ‘collection’: ‘HLSS30.v1.5,’ ‘geometry’: {‘type’: ‘Polygon,’ ‘coordinates’: [[[-119.1488671, 33.3327671], [-118.9832795, 33.3355226], [-118.6783731, 34.3301598], [-119.1737801, 34.3223655], [-119.1488671, 33.3327671]]]}, ‘bbox’: [-119.17378, 33.332767, -118.678373, 34.33016], ‘links’: [{‘rel’: ‘self,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5/items/G1969487860-LPCLOUD’}, {‘rel’: ‘parent,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5’}, {‘rel’: ‘collection,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5’}, {‘rel’: ‘root,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/’}, {‘rel’: ‘provider,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD’}, {‘rel’: ‘via,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.json’}, {‘rel’: ‘via,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.umm_json’}], ‘properties’: {‘datetime’: ‘2015-08-26T18:54:35.450Z,’ ‘start_datetime’: ‘2015-08-26T18:54:35.450Z,’ ‘end_datetime’: ‘2015-08-26T18:54:35.450Z,’ ‘eo:cloud_cover’: 6}, ‘assets’: {‘VZA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.VZA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.VZA.tif’}, ‘VAA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.VAA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.VAA.tif’}, ‘SAA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.SAA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.SAA.tif’}, ‘B11’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B11.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B11.tif’}, ‘B02’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B02.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B02.tif’}, ‘B09’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B09.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B09.tif’}, ‘B12’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B12.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B12.tif’}, ‘B03’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B03.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B03.tif’}, ‘B01’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B01.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B01.tif’}, ‘B07’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B07.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B07.tif’}, ‘SZA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.SZA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.SZA.tif’}, ‘B05’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B05.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B05.tif’}, ‘B06’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B06.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B06.tif’}, ‘Fmask’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.Fmask.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.Fmask.tif’}, ‘B10’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B10.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B10.tif’}, ‘B08’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B08.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B08.tif’}, ‘B8A’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B8A.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B8A.tif’}, ‘B04’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B04.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B04.tif’}, ‘browse’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.jpg,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-public/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.jpg,’ ‘type’: ‘image/jpeg’}, ‘metadata’: {‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.xml,’ ‘type’: ‘application/xml’}}}\n\n\n\n\nSTAC metadata provides valuable information on the item, including a unique ID, when it was acquired, the location of the observation, and a cloud cover assessment.\n\n# Print metadata attributes from this observation\nprint(f\"The ID for this item is: {s30_item['id']}\")\nprint(f\"It was acquired on: {s30_item['properties']['datetime']}\")\nprint(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\nprint(f\"It contains {len(s30_item['assets'])} assets\")\nprint(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n\nThe ID for this item is: G1969487860-LPCLOUD\nIt was acquired on: 2015-08-26T18:54:35.450Z\nover: [-119.17378, 33.332767, -118.678373, 34.33016] (Lower Left, Upper Right corner coordinates)\nIt contains 20 assets\nand is 6% cloudy.\n\n\n\n\nBelow, print out the ten items and the percent cloud cover–we will use this to decide which item to visualize in the next section.\n\nfor i, s in enumerate(s30_items_response['features']):\n    print(f\"Item at index {i} is {s['properties']['eo:cloud_cover']}% cloudy.\")\n\nItem at index 0 is 6% cloudy.\nItem at index 1 is 100% cloudy.\nItem at index 2 is 30% cloudy.\nItem at index 3 is 67% cloudy.\nItem at index 4 is 99% cloudy.\nItem at index 5 is 24% cloudy.\nItem at index 6 is 15% cloudy.\nItem at index 7 is 3% cloudy.\nItem at index 8 is 6% cloudy.\nItem at index 9 is 6% cloudy.\n\n\n\n\nUsing the information printed above, set the item_index below to whichever observation is the least cloudy above.\n\nitem_index = 9  # Indexing starts at 0 in Python, so here select the eighth item in the list at index 7\n\n\ns30_item = s30_items_response['features'][item_index]  # Grab the next item in the list\n\nprint(f\"The ID for this item is: {s30_item['id']}\")\nprint(f\"It was acquired on: {s30_item['properties']['datetime']}\")\nprint(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\nprint(f\"It contains {len(s30_item['assets'])} assets\")\nprint(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n\nThe ID for this item is: G2010297093-LPCLOUD\nIt was acquired on: 2016-11-06T08:21:39.880Z\nover: [26.999803, -24.501758, 28.083499, -23.506303] (Lower Left, Upper Right corner coordinates)\nIt contains 20 assets\nand is 6% cloudy.\n\n\n\n\nBelow, print out the names of all of the assets included in this item.\n\nprint(\"The following assets are available for download:\")\nfor a in s30_item['assets']: print(a)\n\nThe following assets are available for download:\nB05\nB09\nVZA\nB06\nB08\nB10\nB03\nB11\nB07\nFmask\nB04\nB12\nVAA\nSAA\nB01\nB02\nB8A\nSZA\nbrowse\nmetadata\n\n\n\n\nNotice that each HLS item includes a browse image. Read the browse file into memory and visualize the HLS acquisition.\n\ns30_item['assets']['browse']\n\n{‘title’: ‘Download HLS.S30.T35KNP.2016311T080122.v1.5.jpg,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-public/HLSS30.015/HLS.S30.T35KNP.2016311T080122.v1.5.jpg,’ ‘type’: ‘image/jpeg’}\n\n\n\n\nUse the skimage package to load the browse image into memory and matplotlib to quickly visualize it.\n\nimage = io.imread(s30_item['assets']['browse']['href'])  # Load jpg browse image into memory\n\n# Basic plot of the image\nplt.figure(figsize=(10,10))              \nplt.imshow(image)\nplt.show()\n\n\n\n\n\n\nCongrats! You have visualized your first Cloud-Native HLS asset!"
  },
  {
    "href": "files/discovery/index.html",
    "title": "NASA Cloud Data Discovery",
    "section": "",
    "text": "Some background here about discovery.\nSome great text about CMR and CMR-STAC, among other things"
  },
  {
    "href": "files/discovery/earthdata-search-ui.html",
    "title": "Earthdata Search UI",
    "section": "",
    "text": "Maybe a video tutorial?"
  },
  {
    "href": "files/discovery/cmr-virtual-directories.html",
    "title": "CMR Virtual Directories",
    "section": "",
    "text": "Details from here:\nhttps://cmr.earthdata.nasa.gov/search/site/collections/directory/eosdis"
  },
  {
    "href": "files/examples/python-example.html#this-is-a-.ipynb-file",
    "title": "EarthData Cloud Cookbook",
    "section": "This is a .ipynb file",
    "text": "I will write overview and background here\n\n1+1\n2+2\n\n4\n\n\nIf I’m working on narrative, can set up caching so that it doesn’t execute the code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\ntext boxes like this one are in markdown"
  },
  {
    "href": "files/examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "End-to-end workflow examples, linked to pieces from examples above and vice-versa"
  },
  {
    "href": "files/get-started/authentication.html",
    "title": "Authentication",
    "section": "",
    "text": "There are multiple ways to authenticate. Options are:…\nThe following are example notebooks (.ipynb)…"
  },
  {
    "href": "files/get-started/api-primer.html",
    "title": "API Primer",
    "section": "",
    "text": "“Primer” about Interacting with NASA Earthdata APIs"
  },
  {
    "href": "files/get-started/lpdaac-netrc.html#this-is-otherwise-just-testing",
    "title": "LP DAAC Authentication Example",
    "section": "This is otherwise just testing ",
    "text": "Import the required packages and set the input/working directory to run this Jupyter Notebook locally.\n\nimport requests as r\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n\nprint(1+1)\nprint(\"woo NASA data!\")\n\n2\nwoo NASA data!"
  },
  {
    "href": "files/get-started/index.html#prereqsbackground",
    "title": "Getting Started",
    "section": "Prereqs/background",
    "text": "For programmatic, we expect familiarity with XYZ (Python, commandline?, R?). We recommend the following:\n\nIntroduction to Python for Atmospheric Science & Meteorology\nAn Introduction to Earth and Environmental Data Science by Ryan Abernathy and Kerry Key.\n…"
  },
  {
    "href": "files/get-started/index.html#cloud-optimized-data-formats",
    "title": "Getting Started",
    "section": "Cloud Optimized Data Formats",
    "text": "Some great info here about Cloud Optimized Data Formats."
  },
  {
    "href": "files/get-started/index.html#sw3-buckets-etc",
    "title": "Getting Started",
    "section": "SW3 Buckets, etc",
    "text": ""
  },
  {
    "href": "files/get-started/how-to-use.html",
    "title": "How to use this book",
    "section": "",
    "text": "Draft prose here by Andy Barrett.\nIncludes both building blocks and end-to-end workflow examples that put together some of those building blocks. User can ‘choose their own adventure’ and build their own workflows based on their need."
  },
  {
    "href": "files/get-started/earthdata-login.html",
    "title": "EarthData Login",
    "section": "",
    "text": "To access NASA data, you need EarthData Login…\n\nPrevious notes/ideas\nTODO: develop as prose to set up for the following .ipynb examples\nTo access NASA data you have to authenticate.\nSolutions that work - these are detailed in separate chapters as Jupyter notebooks (.ipynb).\n\nTo access NASA data one must setup an Earthdata Login profile. This involves (prose here)\n\nSee/link to Christine’s post & conversation on the Jupyter discourse forum\n\nCreate a netrc file\n\nSubmit EDL credentials within a script\n\nSome talk about the redirects…"
  }
]