---
title: "Access Cloud-Optimized GeoTIFF"
execute:
  eval: false
  echo: fenced
---

## Introduction

We can access Cloud-Optimized GeoTIFF (COG) data directly from NASA Earthdata Cloud using the following code.


## Code

Here are our recommended approaches for accessing COG data in NASA Earthdata Cloud with code.

::: {.panel-tabset group="language"}
## Python

We will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (10.5067/HLS/HLSL30.002) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.

We will access a single COG file, L30 red band (0.64 – 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an xarray dataarray. This approach leverages S3 native protocols for efficient access to the data.

In Python we can use the [`earthdata`](https://earthdata.readthedocs.io/en/latest/) library (to be renamed `earthaccess` very soon!)

To install the package we'll run this code from the command line. Note: you can run shell code directly from a Jupyter Notebook cell by adding a `!`: `!conda install`.

```{bash}
# Install earthdata
conda install -c conda-forge earthdata
```


```{python}
## Import packages
from earthdata import Auth, Store, DataGranules, DataCollections
import requests
import os
import boto3
from osgeo import gdal
import rasterio as rio
from rasterio.session import AWSSession
import rioxarray
import hvplot.xarray
import holoviews as hv
```

### Earthdata Login Authentication

An Earthdata Login account is required to access data from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.

We will authenticate below using a netrc file. See the (TBD) appendix for more information on netrc setup.

```{python}
auth = Auth().login(strategy="netrc")
# are we authenticated?
if not auth.authenticated:
    # ask for credentials and persist them in a .netrc file
    auth.login(strategy="interactive", persist=True)

# The Store class will let us download data from NASA directly
store = Store(auth)
```

### Working with the URLs directly
If we choose, we can use `earthaccess` to grab the file's URLs and then access them with another library. Getting the links to our data is quiet simple with the `data_links()` method on each of the results. See the previous Find Data How-To for more information on how to discover datasets of interest. 


```{python}
#Searching over a small plot in Nebraska, USA over two weeks in September 2022
granules = DataGranules().concept_id("C2021957657-LPCLOUD").temporal("2022-09-10","2022-09-24").bounding_box(-101.67271,41.04754,-101.65344,41.06213).get()
print(len(granules))
granules[0].data_links(access="direct")
```
### Get Temporary AWS Credentials

Direct S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs:

```{python}
s3_cred_endpoint = {
    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',
    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',
    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',
    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',
    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'
}

def get_temp_creds(provider):
    return requests.get(s3_cred_endpoint[provider]).json()

temp_creds_req = get_temp_creds('lpdaac')
```
Create a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.


```{python}
session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], 
                        aws_secret_access_key=temp_creds_req['secretAccessKey'],
                        aws_session_token=temp_creds_req['sessionToken'],
                        region_name='us-west-2')
```
GDAL environment variables must be configured to access COGs in Earthdata Cloud:

```{python}
rio_env = rio.Env(AWSSession(session),
                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',
                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),
                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))
rio_env.__enter__()
```

### Direct In-region Access

Read in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using `rioxarray`, an extension of `xarray` used to read geospatial data. The file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we'll use the squeeze() function to remove band as a dimension.

```{python}
s3_url = granules[0].data_links(access="direct")[8]
da = rioxarray.open_rasterio(s3_url)
da_red = da.squeeze('band', drop=True)
da_red
```
Plot the `dataarray`, representing the L30 red band, using `hvplot`.

```{python}
da_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')
```
Exit the context manager.
```{python}
rio_env.__exit__()
```


## R

R code coming soon!

```{r}
# Coming soon!
```

## Matlab

Matlab code coming soon!

```{bash}
#| echo: true
# Coming soon!
```

## Command Line

With `wget` and `curl`:

```{bash}
# Coming soon!
```


:::
