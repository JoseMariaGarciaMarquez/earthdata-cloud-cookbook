{
  "hash": "94822b89a39700a61886b003e8da71bf",
  "result": {
    "markdown": "---\ntitle: \"Access Cloud-Optimized GeoTIFF\"\nformat: html\neditor: visual\nexecute: \n  eval: false\n---\n\n\n## Summary\n\nWe will access data for the Harmonized Landsat Sentinel-2 (HLS) Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30m v2.0 (L30) (\\[10.5067/HLS/HLSL30.002\\](https://doi.org/10.5067/HLS/HLSL30.002)) data product. These data are archived and distributed as Cloud Optimized GeoTIFF (COG) files, one file for each spectral band.\n\nWe will access a single COG file, L30 red band (0.64 -- 0.67 μm), from inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an `xarray` `dataarray`. This approach leverages S3 native protocols for efficient access to the data.\n\n\n## Code\n\nHere are our recommended approaches for accessing COG data in NASA Earthdata Cloud with code.\n\n### Import Packages\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\nIn Python we can use the [`earthaccess`](https://nsidc.github.io/earthaccess/) library.\n\nTo install the package we'll run this code from the command line. Note: you can run shell code directly from a Jupyter Notebook cell by adding a `!`: `!conda install`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Install earthaccess\nconda install -c conda-forge earthaccess\n```\n:::\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nimport earthaccess\nimport requests\nimport os\nimport boto3\nfrom osgeo import gdal\nimport rasterio as rio\nfrom rasterio.session import AWSSession\nimport rioxarray\nimport hvplot.xarray\nimport holoviews as hv\n\n#From Mahsa's tutorial in main:\n#import os\n#from osgeo import gdal\n#import rasterio as rio\n#import rioxarray\n#import hvplot.xarray\n#import holoviews as hv\n```\n:::\n\n\n## R\n\n\n::: {.cell filename='[R code]'}\n\n```{.r .cell-code}\nlibrary(rgdal)\nlibrary(raster)\nlibrary(terra)\n```\n:::\n\n:::\n\n## Workspace Environment Setup\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\nFor this exercise, we are going to open up a context manager for the notebook using the rasterio.env module to store the required GDAL configurations we need to access the data from Earthdata Cloud. While the context manager is open (rio_env.\\_\\_enter\\_\\_()) we will be able to run the open or get data commands that would typically be executed within a with statement, thus allowing us to more freely interact with the data. We'll close the context (rio_env.\\_\\_exit\\_\\_()) at the end of the notebook.\n\nGDAL environment variables must be configured to access COGs from Earthdata Cloud. Geospatial data access Python packages like rasterio and rioxarray depend on GDAL, leveraging GDAL's \"Virtual File Systems\" to read remote files. GDAL has a lot of environment variables that control it's behavior. Changing these settings can mean the difference being able to access a file or not. They can also have an impact on the performance.\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nrio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n```\n:::\n\n\n## R\n\nSet up rgdal configurations to access the cloud assets that we are interested in. You can learn more about these configuration options here.\n\n\n::: {.cell filename='[R code]'}\n\n```{.r .cell-code}\nsetGDALconfig(c(\"GDAL_HTTP_UNSAFESSL=YES\",\n                \"GDAL_HTTP_COOKIEFILE=.rcookies\",\n                \"GDAL_HTTP_COOKIEJAR=.rcookies\",\n                \"GDAL_DISABLE_READDIR_ON_OPEN=EMPTY_DIR\",\n                \"CPL_VSIL_CURL_ALLOWED_EXTENSIONS=TIF\"))\n```\n:::\n\n:::\n\nIn this example we're interested in the HLS L30 data collection from NASA's LP DAAC in Earthdata Cloud. Below we specify the HTTPS URL to the data asset in Earthdata Cloud. This URL can be found via Earthdata Search or programmatically through the CMR and CMR-STAC APIs.\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nhttps_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n```\n:::\n\n\n## R\n\nPlease note that in R, we need to add `/vsicurl/` manually to the COG file URL.\n\n\n::: {.cell filename='[R code]'}\n\n```{.r .cell-code}\nhttps_url <- '/vsicurl/https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSL30.020/HLS.L30.T11SQA.2021333T181532.v2.0/HLS.L30.T11SQA.2021333T181532.v2.0.B04.tif'\n```\n:::\n\n:::\n\n\n\n## HTTPS Data Access\n\nRead in the HLS HTTPS URL for the L30 red band (0.64 -- 0.67 μm) into our workspace. Note that, accessing files in the cloud requires you to authenticate using your NASA Earthdata Login account meaning a proper netrc file needs to be set up.\n\n\n::: {.panel-tabset group=\"language\"}\n## Python\n\nWe will authenticate below using a netrc file. See the (TBD) appendix for more information on netrc setup.\n\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nauth = Auth().login(strategy=\"netrc\")\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)\n\n# The Store class will let us download data from NASA directly\nstore = Store(auth)\n```\n:::\n\n\n### Working with the URLs directly\nIf we choose, we can use `earthaccess` to grab the file's URLs and then access them with another library. Getting the links to our data is quiet simple with the `data_links()` method on each of the results. See the previous Find Data How-To for more information on how to discover datasets of interest. \n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\n#Searching over a small plot in Nebraska, USA over two weeks in September 2022\ngranules = DataGranules().concept_id(\"C2021957657-LPCLOUD\").temporal(\"2022-09-10\",\"2022-09-24\").bounding_box(-101.67271,41.04754,-101.65344,41.06213).get()\nprint(len(granules))\ngranules[0].data_links(access=\"direct\")\n```\n:::\n\n\n### Get Temporary AWS Credentials\n\nDirect S3 access is achieved by passing NASA supplied temporary credentials to AWS so we can interact with S3 objects from applicable Earthdata Cloud buckets. For now, each NASA DAAC has different AWS credentials endpoints. Below are some of the credential endpoints to various DAACs.\n\nCOMING SOON: We can use the `earthaccess` `store` class to pass these credentials directly to Boto3 without the need to set up this function.\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\ns3_cred_endpoint = {\n    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n}\n\ndef get_temp_creds(provider):\n    return requests.get(s3_cred_endpoint[provider]).json()\n\ntemp_creds_req = get_temp_creds('lpdaac')\n```\n:::\n\nCreate a boto3 Session object using your temporary credentials. This Session is used to pass credentials and configuration to AWS so we can interact wit S3 objects from applicable buckets.\n\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nsession = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n                        aws_session_token=temp_creds_req['sessionToken'],\n                        region_name='us-west-2')\n```\n:::\n\nGDAL environment variables must be configured to access COGs in Earthdata Cloud:\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nrio_env = rio.Env(AWSSession(session),\n                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\nrio_env.__enter__()\n```\n:::\n\n\n### Direct In-region Access\n\nRead in the HLS s3 URL for the L30 red band (0.64 – 0.67 μm) into our workspace using `rioxarray`, an extension of `xarray` used to read geospatial data. The file is read into Python as an xarray dataarray with a band, x, and y dimension. In this example the band dimension is meaningless, so we'll use the squeeze() function to remove band as a dimension.\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\ns3_url = granules[0].data_links(access=\"direct\")[8]\nda = rioxarray.open_rasterio(s3_url)\nda_red = da.squeeze('band', drop=True)\nda_red\n```\n:::\n\nPlot the `dataarray`, representing the L30 red band, using `hvplot`.\n\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nda_red.hvplot.image(x='x', y='y', cmap='gray', aspect='equal')\n```\n:::\n\nExit the context manager.\n\n::: {.cell filename='[python code]'}\n\n```{.python .cell-code}\nrio_env.__exit__()\n```\n:::\n\n\n\n## R\n\n\n::: {.cell filename='[R code]'}\n\n```{.r .cell-code}\nda_red <- rast(https_url)\nda_red\n```\n:::\n\n\nThe Convert a `SpatRaster` object to a `Raster` object using raster() to be able to use `leaflet` to plot our data.\n\n\n::: {.cell filename='[R code]'}\n\n```{.r .cell-code}\nred_raster <- da_red %>% raster()\nred_raster\n```\n:::\n\n\nThen plot the `red band` using `plot` function.\n\n\n::: {.cell filename='[R code]'}\n\n```{.r .cell-code}\nplot(red_raster)\n```\n:::\n\n\n## Matlab\n\nMatlab code coming soon!\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Coming soon!\n```\n:::\n\n\n## Command Line\n\nWith `wget` and `curl`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Coming soon!\n```\n:::\n\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}