{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d00560f9-1def-4210-b97c-34de75e49f0a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"NASA Earthdata Cloud Clinic\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf218d52-4c89-48a1-bbb0-567e2378e0db",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook will walk through two different access and transformation options across a single dataset available in the Earthdata Cloud: \n",
    "1. Direct s3 access using the [earthaccess](https://github.com/nsidc/earthaccess) python library\n",
    "2. [Harmony](https://harmony.earthdata.nasa.gov/) transformation services \n",
    "\n",
    "These steps are based off of several notebook tutorials presented during the [2021 Earthdata Cloud Hackathon](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/) and [2021 AGU Workshop](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/), as well as NSIDC DAAC's [Accessing and working with ICESat-2 data in the cloud](https://github.com/nsidc/NSIDC-Data-Tutorials/tree/main/notebooks/ICESat-2_Cloud_Access) tutorial.\n",
    "\n",
    "TODO: Describe dataset, ideally with Worldview image if available. Need to decide whether to stick with MODIS or MUR data (higher resolution but difficult to load into memory in small instance).\n",
    "\n",
    "We will access a single NetCDF file from the GHRSST Level 4 MUR Global Foundation Sea Surface Temperature Analysis (v4.1) dataset (MUR-JPL-L4-GLOB-v4.1) inside the AWS cloud (us-west-2 region, specifically) and load it into Python as an `xarray` `dataset` using the `earthaccess` python library. \n",
    "\n",
    "After inspecting the data, we will then access the same file, but in this case we will also subset the data to our area of interest using Harmony. \n",
    "\n",
    "## Learning Objectives (TODO: Need to update)\n",
    "\n",
    "- how to retrieve temporary S3 credentials for in-region direct S3 bucket access\n",
    "- how to define a dataset of interest and find netCDF files in S3 bucket\n",
    "- how to perform in-region direct access of MODIS_T-JPL-L2P-v2019.0 data in S3\n",
    "- how to plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962cdc1-5bfc-42ad-b522-e870822f9540",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c1753-d01d-41de-bc0e-cf0284b74fcc",
   "metadata": {},
   "source": [
    "# 0. Cloud Environment Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01279402-f790-4fcc-85df-8e50b168d8eb",
   "metadata": {},
   "source": [
    "## Step 1. Login to the Hub\n",
    "\n",
    "Please go to the [NASA Openscapes 2i2c JupyterHub](https://openscapes.2i2c.cloud/) and click \"Log in to continue\", which will bring us to the CILogon page. With \"Github\" selected as the Identity Provider, click \"Log On\" and sign in with your GitHub account. We then have the option to select the server type and size. For this tutorial, we will select the default options: \"Python\" and \"Small\". Select \"Start\" to spin up the cloud environment instance. \n",
    "\n",
    "*Note: It may take a few moments for the Hub to load. Please be patient!*\n",
    "\n",
    "TODO: Include screenshots here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d39eb1-dd5b-4250-8d08-174ae4495bd7",
   "metadata": {},
   "source": [
    "## Step 2. JupyterLab orientation\n",
    "\n",
    "Once the Hub is loaded, we have access to a JupyterLab interface. \n",
    "\n",
    "### First impressions\n",
    "* Home directory on the lefthand navigation panel\n",
    "* Blue \"Plus\" button to open the Launcher\n",
    "    * Create new files including Jupyter Notebook (.ipynb)\n",
    "    * Open Terminal window to interact on the command line\n",
    "    * Restarting the Kernel under \"Kernel\" menu at the top of the screen. \n",
    "\n",
    "TODO: Include screenshots here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c338c3d-fe47-461f-bd7d-dee07ae97e88",
   "metadata": {},
   "source": [
    "## Step 3. Navigate to the Earthdata Cookbook folder\n",
    "The folder `earthdata-cloud-cookbook` is in the /shared folder on JupyterHub.\n",
    "\n",
    "TODO: Is this up to date? If not, how do we update it?\n",
    "\n",
    "## Jupyter notebooks\n",
    "Let’s get oriented to Jupyter notebooks, which we’ll use for this clinic.\n",
    "TODO: Add a few examples of how to work with notebooks\n",
    "\n",
    "## How do I end my session?\n",
    "(Also see [How do I end my Openscapes session? Will I lose all of my work?](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/clinic/jupyterhub.html#how-do-i-end-my-openscapes-session))\n",
    "\n",
    "When you are finished working for the day it is important to explicitly log out of your Openscapes session. The reason for this is it will save us a bit of money! When you keep a session active it uses up AWS resources and keeps a series of virtual machines deployed.\n",
    "\n",
    "Stopping the server happens automatically when you log out, so navigate to “File -> Log Out” and just click “Log Out”!\n",
    "\n",
    "!!! NOTE “logging out” - Logging out will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.\n",
    "\n",
    "TODO: Can we do some fancy Quarto styling here with an info graphic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d91be6-0dd1-4f3d-964e-c51393e95260",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488de1c-9177-405a-b957-92045aad16ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Prerequisites\n",
    "\n",
    "## AWS instance running in us-west-2\n",
    "\n",
    "NASA Earthdata Cloud data in S3 can be directly accessed via temporary credentials; this access is limited to requests made within the US West (Oregon) (code: us-west-2) AWS region.\n",
    "\n",
    "TODO: Back up and walk through what S3 is and the fact that the Hub is running in us-west-2\n",
    "\n",
    "## Earthdata Login\n",
    "\n",
    "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit <https://urs.earthdata.nasa.gov> to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e76663-3cee-44c0-b158-03d482f8a534",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb418f5-09e1-4841-a608-66e706da42bb",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6fad23-fa66-4e3d-818b-96f797025b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# earthaccess\n",
    "import earthaccess \n",
    "\n",
    "# Direct access\n",
    "\n",
    "#import requests\n",
    "import s3fs\n",
    "from pprint import pprint\n",
    "import xarray as xr\n",
    "\n",
    "# Harmony\n",
    "\n",
    "from harmony import BBox, Client, Collection, Request, LinkType\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8a614-de01-4905-949c-17588d9a623f",
   "metadata": {},
   "source": [
    "## 1. Authentication for NASA Earthdata \n",
    "\n",
    "The first step is to get the correct authentication that will allow us to get cloud-hosted data from NASA. This is all done through Earthdata Login. The `login` method also gets the correct AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938cefcd-63e5-4118-8500-45e74d6dfc89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Earthdata Login username:  amy.steiker\n",
      "Enter your Earthdata password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're now authenticated with NASA Earthdata Login\n",
      "Using token with expiration date: 06/16/2023\n",
      "Using user provided credentials for EDL\n",
      "Persisting credentials to .netrc\n"
     ]
    }
   ],
   "source": [
    "auth = earthaccess.login(strategy=\"interactive\", persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fcbf7-37cc-477e-be92-3d767e06dbe4",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Accessing a NetCDF4 - S3 Direct Access\n",
    "\n",
    "### Summary\n",
    "\n",
    "With earthaccess we can login, search and download data with a few lines of code and even more relevant, our code will work the same way if we are running it in the cloud or from our laptop. earthaccess handles authentication with NASA's Earthdata Login (EDL), search using NASA's CMR and access through fsspec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab966f-d1c8-4293-9d26-82e8ecb53b8c",
   "metadata": {},
   "source": [
    "### Search for data\n",
    "\n",
    "`earthaccess` leverages the Common Metadata Repository (CMR) API to search for collections and granules. Earthdata Search also uses the CMR API.\n",
    "\n",
    "We can use the keyword method for collection_query to search for ICESat-2 collections.\n",
    "\n",
    "TODO: Can I search by hour below?\n",
    "\n",
    "TODO: I use DOI to search by dataset `earthaccess.search_datasets()` but not if I switch to `earthaccess.search_data`. I don't fully understand the difference between the two and when I would use these vs dataset and granule queries (`earthaccess.granule_query()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d096dc-c8ee-44d3-a130-b1dc5d9ae3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_name=\"MUR-JPL-L4-GLOB-v4.1\"\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    #doi=\"10.5067/GHGMR-4FJ04\",\n",
    "    short_name=short_name,\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2021-03-10\", \"2021-03-10\"),\n",
    "    #temporal=(\"2021-03-10T10:00:00Z\", \"2021-03-10T10:00:00Z\"),\n",
    "    bounding_box=(-125.469,15.820,-99.453,35.859)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78660efc-0566-4135-a692-078b9fb6bbc8",
   "metadata": {},
   "source": [
    "Discover information about the matching files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae10e87-e1da-4a73-92e8-6ce7de141243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f9971-a344-4a18-b3fc-8ef250be99e4",
   "metadata": {},
   "source": [
    "### Direct In-region Access\n",
    "\n",
    "Stream data directly to xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e178e1-cea1-497a-942d-05091a2e82b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(earthaccess.open(results))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbb49d-dbdc-473f-97d3-6733ef2f6199",
   "metadata": {},
   "source": [
    "Note: Plotting is too much for the small instance (I think I saw ~20 GB of memory going before it crashed). But let's try to extract just the time and spatial range of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657bbde-c9a9-42f1-ae76-c8dd7724b354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset = ds['analysed_sst'].sel(lat=slice(15.820, 35.859), lon=slice(-125.469,-99.453))\n",
    "ds_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6319e50-566e-4d94-885c-6afada573d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_subset.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf7721-3a70-4dab-ac66-16fcf3fc8cf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b8d37-cc33-48ec-9145-a702dde0ee79",
   "metadata": {},
   "source": [
    "## 3. Accessing Multiple NetCDF4 Files - Data Subsetting and Transformation Services in the Cloud\n",
    "\n",
    "### Using the Harmony-Py library to access customized data from NASA Earthdata \n",
    "\n",
    "#### What other access options or services exist for this dataset?\n",
    "\n",
    "Maybe we're interested in creating a time series over a larger area or with a larger dataset. Let's see whether there are other services out there that could either make this operation more performant in the cloud (with a cloud-optimized output) or subsetting services to reduce the data volume.\n",
    "\n",
    "Based off of https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/07_Harmony_Subsetting.html \n",
    "\n",
    "### Summary\n",
    "\n",
    "We have already explored direct access to the NASA EOSDIS archive in the cloud via the Amazon Simple Storage Service (S3) by using the Common Metadata Repository (CMR) to search for granule locations. In addition to directly accessing the files archived and distributed by each of the NASA DAACs, many datasets also support services that allow us to customize the data via subsetting, reformatting, reprojection, and other transformations. \n",
    "\n",
    "This tutorial demonstrates how to find, request, and use customized data from a new ecosystem of services operating within the NASA Earthdata Cloud: NASA Harmony.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "But first, why use this option when we've already learned how to access data directly from the NASA Earthdata Cloud? \n",
    "\n",
    "- Consistent access patterns to EOSDIS holdings make cross-data center data access easier\n",
    "- Data reduction services allow us to request only the data we want, in the format and projection we want\n",
    "- Analysis Ready Data and cloud access will help reduce time-to-science\n",
    "- Community Development helps reduce the barriers for re-use of code and sharing of domain knowledge\n",
    "\n",
    "See more on the [Earthdata Harmony landing page](https://harmony.earthdata.nasa.gov/), including documentation on the Harmony API itself. \n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Conceptualize the data transformation service types and offerings provided by NASA Earthdata, including Harmony.\n",
    "2. Practice skills learned from the introductory CMR tutorial to discover what access and service options exist for a given data set, as well as variable metadata.\n",
    "3. Utilize the Harmony-py library to request subsetted MODIS L2 Sea Surface Temperature data over the Gulf of Mexico. \n",
    "4. Read Harmony subsetted outputs directly into xarray. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2cbcb9-6050-4e65-a4cb-528b45c4d693",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discover service options for a given data set\n",
    "\n",
    "#### _First, what do we mean by a \"service\"?_\n",
    "\n",
    "In the context of NASA Earthdata, we are usually referring to a service as any data transformation or customization process that packages or delivers data in a way that makes it easier to work with compared to how the data are natively archived at NASA EOSDIS. Basic customization options may include:\n",
    "* Subsetting (cropping) the data by:\n",
    "    * Variable\n",
    "    * Spatial boundary,\n",
    "    * Temporal range\n",
    "* Reformatting\n",
    "    * For example: From NetCDF-4 to Cloud Optimized GeoTIFF\n",
    "* Reprojection and/or Resampling\n",
    "    * For example: From Sinusoidal to Polar Stereographic\n",
    "* Mosaicking\n",
    "* Aggregating\n",
    "\n",
    "A few main types or pathways for services that are commonly supported across the NASA DAACs include:\n",
    "* [NASA Global Imagery Browse Service](https://earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/gibs)\n",
    "    * Web services providing imagery, much of which is updated daily, to broaden accessibility of NASA EOSDIS data to the media and public.\n",
    "* [OPeNDAP](https://opendap.earthdata.nasa.gov/)\n",
    "    * The Open-source Project for a Network Data Access Protocol is a NASA community standard DAP that provides a simple way to access and work with data over the internet. OPeNDAP's client/server software allows us to subset and reformat data using an internet browser, command line interface, and other applications.\n",
    "* [Harmony](https://harmony.earthdata.nasa.gov/)\n",
    "    * In the most basic sense, Harmony is an Application Programming Interface, or API, allowing us to request customization options described above, which are then processed and returned as file outputs. Harmony helps to reduce pre-processing steps so we can spend less time preparing the data, and more time doing science.  \n",
    "\n",
    "**Note: These service offerings are unique to each NASA EOSDIS dataset.**\n",
    "\n",
    "Why is this?\n",
    "\n",
    "Due to varying [levels of service](https://earthdata.nasa.gov/collaborate/new-missions/level-of-service), cloud migration status, and unique characteristics of the datasets themselves, not all service options are provided for all datasets. Therefore it is important to first explore a given dataset's metadata to discover what service options are provided.\n",
    "\n",
    "#### Note that the full [Harmony tutorial](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/07_Harmony_Subsetting.html) from the 2021 Earthdata Cloud Hackathon demonstrates service and variable discovery, but this was removed here for simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce6fc6-1f2c-45a0-a824-c67f29a83e35",
   "metadata": {},
   "source": [
    "## Using Harmony-Py to subset data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b8f0e-848c-4537-ad26-4af3a0251280",
   "metadata": {},
   "source": [
    "Harmony-Py provides a pip installable Python alternative to directly using Harmony's RESTful API to make it easier to request data and service options, especially when interacting within a Python Jupyter Notebook environment.\n",
    "\n",
    "The next steps are adopted from the [introduction tutorial](https://github.com/nasa/harmony-py/blob/main/examples/intro_tutorial.ipynb) notebook provided in the Harmony-Py library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d05f77-8c21-4c7e-b865-b191398f93ae",
   "metadata": {},
   "source": [
    "### Create Harmony Client object\n",
    "First, we need to create a Harmony Client, which is what we will interact with to submit and inspect a data request to Harmony, as well as to retrieve results.\n",
    "\n",
    "When creating the Client, we need to provide Earthdata Login credentials, which are required to access data from NASA EOSDIS. This basic line below assumes that we have a `.netrc` available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387b657-10aa-4503-8454-3a7438734dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f5aef-8427-4ae7-8238-8c8adee8347d",
   "metadata": {},
   "source": [
    "### Create Harmony Request\n",
    "\n",
    "The following are common request parameters:\n",
    "\n",
    "- `collection`: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n",
    "    - Provide a concept ID (e.g. C1940473819-POCLOUD)\n",
    "    - Data product short name (e.g. MODIS_A-JPL-L2P-v2019.0).\n",
    "- `spatial`: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\n",
    "- `temporal`: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n",
    "\n",
    "As we identified above, only subsetting options are available for this dataset. If other service options such as reformatting are available for a given dataset, these can also be specified using Harmony-py: See the [documentation](https://harmony-py.readthedocs.io/en/latest/) for details on how to construct these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f1aa6-3b71-4b6f-a321-3c9d7eef8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(\n",
    "    collection=Collection(id=short_name),\n",
    "    spatial=BBox(-125.469,15.820,-99.453,35.859),\n",
    "    temporal={\n",
    "    'start': dt.datetime(2021, 3, 10, 1),\n",
    "    'stop': dt.datetime(2021, 3, 10, 2)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc83c1-a2ea-41bb-b319-48fde922bf8d",
   "metadata": {},
   "source": [
    "### Submit request\n",
    "\n",
    "Now that the request is created, we can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents the submitted request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe541923-f17e-4027-831b-e35004b1c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = harmony_client.submit(request)\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e74e62-aacd-4dff-ac45-5e9f322f0677",
   "metadata": {},
   "source": [
    "### Check request status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a49cb5-6e27-42c6-83a0-0d88f50e9fb8",
   "metadata": {},
   "source": [
    "Depending on the size of the request, it may be helpful to wait until the request has completed processing before the remainder of the code is executed. The wait_for_processing() method will block subsequent lines of code while optionally showing a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414eaaa-0b84-4023-ab72-d49c068c1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_client.wait_for_processing(job_id, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17ff08-1a87-439e-8399-db24b1844c7d",
   "metadata": {},
   "source": [
    "### View Harmony job response and output URLs\n",
    "Once the data request has finished processing, we can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on the request if available.\n",
    "\n",
    "result_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee7ccf-fc54-4f07-9223-f724d968c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = harmony_client.result_json(job_id)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7948bd-433e-428c-a4c0-a9651e8775dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Direct cloud access\n",
    "\n",
    "Note that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\n",
    "\n",
    "Harmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response.\n",
    "\n",
    "#### Retrieve list of output URLs.\n",
    "\n",
    "The result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924317f8-74de-4937-bf91-1e4e408ca9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = harmony_client.result_urls(job_id, link_type=LinkType.s3)\n",
    "urls = list(results)\n",
    "url = urls[0]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b62050-88ad-4c50-9d38-6fabf36dc612",
   "metadata": {},
   "source": [
    "#### Using earthaccess to open URL from Harmony\n",
    "\n",
    "TODO: Can we take Harmony's s3 URL and open directly a la `ds = xr.open_mfdataset(earthaccess.open(results))`? I'm not quite sure how to take a link or list of links and input it so it has the same structure as what you'd get from `earthaccess.open(results)`.\n",
    "\n",
    "Using `aws_credentials` you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86419068-0d82-4cd1-84d1-eb59b699e07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "creds = harmony_client.aws_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a10c1-f72c-46f5-9b22-3b3eabb7b179",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Open staged files with *s3fs* and *xarray*\n",
    "\n",
    "We use the AWS `s3fs` package to create a file system that can then be read by xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3eb41d-d059-4ffb-b2b3-8675f238e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_fs = s3fs.S3FileSystem(\n",
    "    key=creds['aws_access_key_id'],\n",
    "    secret=creds['aws_secret_access_key'],\n",
    "    token=creds['aws_session_token'],\n",
    "    client_kwargs={'region_name':'us-west-2'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b411be-49be-47a5-bad2-e3cceec250fd",
   "metadata": {},
   "source": [
    "Now that we have our s3 file system set, including our declared credentials, we'll use that to open the url, and read in the file through xarray. This extra step is needed because xarray cannot open the S3 location directly. Instead, the S3 file object is passed to xarray, in order to then open the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef6817-ee65-4fd4-9f66-85e51c8102c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = s3_fs.open(url, mode='rb')\n",
    "ds = xr.open_dataset(f)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb218932-4896-4902-8259-054dd10e4ad9",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "Use the xarray built in plotting function to create a simple plot along the x and y dimensions of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0da7c1-066b-4bbc-90d9-f34eca0fb746",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.analysed_sst.plot() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f0870-fe10-4df1-8107-bd8591d86d57",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook demonstrated an abbreviated and simplified workflow to explore access and subsetting options available through the Earthdata Cloud. There are several other options that can be used to work \"in place\" in the Earthdata Cloud, from data discovery to analysis-ready data, including: \n",
    "\n",
    "* [Zarr-EOSDIS-Store](https://github.com/nasa/zarr-eosdis-store)\n",
    "    * The zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently by the Zarr Python library, provided they have a sidecar DMR++ metadata file generated. \n",
    "    * Tutorial highlighting this library's usage: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/09_Zarr_Access.html \n",
    "* [OPeNDAP](https://opendap.earthdata.nasa.gov/) \n",
    "    * Hyrax provides direct access to subsetting of NASA data using Python or your favorite analysis tool\n",
    "    * Tutorial highlighting OPeNDAP usage: https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/06_S6_OPeNDAP_Access_Gridding.html\n",
    "* [Earthaccess Library](https://github.com/nsidc/earthdaccess)\n",
    "    * A Python library to search and access NASA Earthdata datasets, aiming to provide a simple way to access or download data without having to worry if a given dataset is on-prem or in the cloud.\n",
    "    \n",
    "TODO Cleanup: See the [appendix](\"https://nasa-openscapes.github.io/earthdata-cloud-cookbook/appendix/authentication.html\") for more information on [Earthdata Login](\"https://urs.earthdata.nasa.gov/\") and netrc setup. This basic line below to create an earthaccess Client assumes that we have a .netrc available.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe97e1a-b779-42d6-8420-c1cc029b5334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
